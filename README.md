# ğŸ§  Self-Refine CLI â€” Autonomous Self-Improving Agent

<div align="center">

**A fully autonomous, self-improving AI agent that generates code, verifies it, learns from mistakes, and continuously upgrades its own capabilities.**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![llama.cpp](https://img.shields.io/badge/inference-llama.cpp-green.svg)](https://github.com/ggerganov/llama.cpp)
[![Local Hardware](https://img.shields.io/badge/runs%20on-local%20GPU-orange.svg)](#requirements)

</div>

> âš ï¸ **100% LOCAL** â€” Runs entirely on your machine using llama.cpp with Vulkan GPU acceleration. No paid APIs (OpenAI, Anthropic, Google). Perfect for AI experimentation on your own hardware.

---

## ğŸ¯ What This Project Does

This is not just another LLM wrapper. **Self-Refine CLI** is an autonomous agent that:

1. **Generates coding tasks** for itself with adaptive difficulty
2. **Spawns 3 parallel workers** that each generate, execute, and verify code
3. **Selects the best verified solution** from workers
4. **Iteratively refines** the solution using self-feedback
5. **Learns lessons** from successes and failures into long-term memory
6. **Harvests skills** â€” verified functions become reusable for future tasks
7. **Adjusts difficulty** â€” curriculum learning based on performance history

The result: an agent that **improves itself over time** without human intervention.

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           AUTONOMOUS LOOP                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Task Generator â”‚â”€â”€â”€â–¶â”‚ PoetiqRunner   â”‚â”€â”€â”€â–¶â”‚ Result Logger  â”‚        â”‚
â”‚  â”‚ (Adaptive Diff)â”‚    â”‚                â”‚    â”‚ + Learner      â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   POETIQ PIPELINE   â”‚    â”‚   MEMORY SYSTEM     â”‚
        â”‚                     â”‚    â”‚                     â”‚
        â”‚ â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”    â”‚    â”‚  SmartMemory        â”‚
        â”‚ â”‚ W1  â”‚ â”‚ W2  â”‚    â”‚    â”‚  ContextVectors     â”‚
        â”‚ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜    â”‚    â”‚  MemoryGraph        â”‚
        â”‚    â”‚       â”‚       â”‚    â”‚  WorkingMemory      â”‚
        â”‚ â”Œâ”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”    â”‚    â”‚  SkillHarvester     â”‚
        â”‚ â”‚  Aggregator â”‚    â”‚    â”‚  TestPatterns       â”‚
        â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚  ReflectionBuffer   â”‚
        â”‚        â–¼           â”‚    â”‚                     â”‚
        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ â”‚ SelfRefiner â”‚    â”‚
        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+**
- **GPU with Vulkan support** (tested on AMD RX 6600, NVIDIA works too)
- **16GB+ RAM** recommended
- **[llama.cpp server](https://github.com/ggerganov/llama.cpp)** running locally

### 1. Clone & Install

```bash
# Clone the repository
git clone https://github.com/mauro3422/self-refine-cli.git
cd self-refine-cli

# Create virtual environment (recommended)
python -m venv venv
venv\Scripts\activate   # Windows
# source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### 2. Download & Start LLM Server

```bash
# Download a model (example: Qwen2.5-Coder-7B)
# Place in models/ folder

# Start llama.cpp server with Vulkan GPU acceleration
# Windows:
scripts\start_llm.bat

# Linux/Mac:
# ./llama.cpp/build/bin/llama-server -m models/your-model.gguf -c 32768 --port 8080
```

### 3. Run the Agent

```bash
# Autonomous mode - let the agent teach itself
python autonomous_loop.py

# Interactive mode - give tasks directly
python main.py

# Single task with Poetiq pipeline
python run_test.py "create a function that reverses a string" --poetiq
```

### 4. Monitor (Optional)

```bash
# Launch web dashboard at http://localhost:5000
python -m ui.dashboard
```


---

## ğŸ“ Project Structure

```
self-refine-cli/
â”œâ”€â”€ autonomous_loop.py       # ğŸ”„ Main autonomous self-improvement loop
â”œâ”€â”€ main.py                  # ğŸ–¥ï¸ Interactive CLI entry point
â”‚
â”œâ”€â”€ core/                    # âš™ï¸ Core Modules
â”‚   â”œâ”€â”€ poetiq/              # Poetiq Pipeline
â”‚   â”‚   â”œâ”€â”€ runner.py        #   â””â”€ Orchestrates workers â†’ aggregator â†’ refiner
â”‚   â”‚   â”œâ”€â”€ worker.py        #   â””â”€ True Poetiq: generate + execute + verify
â”‚   â”‚   â”œâ”€â”€ aggregator.py    #   â””â”€ Selects best verified response
â”‚   â”‚   â””â”€â”€ refiner.py       #   â””â”€ Self-refine loop with feedback
â”‚   â”œâ”€â”€ llm_client.py        # LLM communication (OpenAI-compatible)
â”‚   â”œâ”€â”€ code_verifier.py     # Execute code against test cases
â”‚   â”œâ”€â”€ agentic_loop.py      # Multi-tool execution loop
â”‚   â”œâ”€â”€ parsers.py           # Extract tool calls, scores, code
â”‚   â””â”€â”€ prompts.py           # Centralized system prompts
â”‚
â”œâ”€â”€ memory/                  # ğŸ§  Memory System (7 subsystems)
â”‚   â”œâ”€â”€ orchestrator.py      # Central hub coordinating all memory
â”‚   â”œâ”€â”€ base.py              # SmartMemory: long-term with decay + ranking
â”‚   â”œâ”€â”€ context_vectors.py   # Category detection + tool suggestions
â”‚   â”œâ”€â”€ llm_linker.py        # Intelligent memory ranking
â”‚   â”œâ”€â”€ graph.py             # NetworkX graph with PageRank
â”‚   â”œâ”€â”€ working_memory.py    # Project file indexing (ChromaDB)
â”‚   â”œâ”€â”€ evolution.py         # Merge/evolve memories
â”‚   â”œâ”€â”€ reflection_buffer.py # Intra-session error avoidance
â”‚   â”œâ”€â”€ learner.py           # Extract lessons from sessions
â”‚   â”œâ”€â”€ skill_harvester.py   # Save verified functions as skills
â”‚   â”œâ”€â”€ test_patterns.py     # Learn successful test patterns
â”‚   â”œâ”€â”€ adaptive_difficulty.py # Curriculum learning (1-5)
â”‚   â”œâ”€â”€ cache.py             # LRU embedding cache
â”‚   â”œâ”€â”€ vector_store.py      # ChromaDB vector storage
â”‚   â””â”€â”€ persistence.py       # Export/import memory state
â”‚
â”œâ”€â”€ tools/                   # ğŸ”§ Agent Tools
â”‚   â”œâ”€â”€ registry.py          # Singleton tool registry
â”‚   â”œâ”€â”€ file_tools.py        # read_file, write_file, list_dir
â”‚   â”œâ”€â”€ code_tools.py        # python_exec
â”‚   â”œâ”€â”€ edit_tools.py        # replace_in_file, apply_patch
â”‚   â”œâ”€â”€ search_tools.py      # search_files
â”‚   â”œâ”€â”€ command_tools.py     # run_command
â”‚   â””â”€â”€ verify_tools.py      # linter, run_tests
â”‚
â”œâ”€â”€ config/                  # âš™ï¸ Configuration
â”‚   â””â”€â”€ settings.py          # All centralized settings
â”‚
â”œâ”€â”€ data/                    # ğŸ’¾ Persistent Data
â”‚   â”œâ”€â”€ agent_memory.json    # Long-term memories
â”‚   â”œâ”€â”€ memory_graph.json    # Memory relationships
â”‚   â”œâ”€â”€ skills/              # Harvested skills library
â”‚   â””â”€â”€ test_patterns/       # Learned test patterns
â”‚
â”œâ”€â”€ sandbox/                 # ğŸ“¦ Secure Execution Environment
â””â”€â”€ output/                  # ğŸ“Š Logs and Session Data
```

---

## ğŸ§  Memory System Deep Dive

The memory system is inspired by **A-Mem** (Agentic Memory) and **DreamCoder**. It enables the agent to:

### 1. SmartMemory â€” Long-Term Lessons
```python
# Memories have:
# - Temporal decay (0.98/day) â€” old unused memories fade
# - Importance scoring (1-10) â€” critical lessons persist
# - Success/failure tracking â€” learns what works
# - Weighted links to related memories
```

### 2. ContextVectors â€” Category Detection
```python
# Detects task type from keywords:
CATEGORIES = ["file_create", "file_read", "code_exec", "analysis", ...]

# Suggests relevant tools:
"file_create" â†’ ["write_file", "python_exec"]
```

### 3. InContextVectors (ICV) â€” Dynamic Tips
```python
# Category-specific guidance injected into prompts:
"code_exec" â†’ "Always include error handling. Test edge cases."
```

### 4. MemoryGraph â€” Relational Knowledge
```python
# NetworkX graph connecting related memories
# Uses PageRank to identify central/important memories
# Strengthens links on co-retrieval, weakens on contradictions
```

### 5. WorkingMemory â€” Project Context
```python
# Indexes current project files using ChromaDB
# Chunks Python files by function/class for precise retrieval
# Provides relevant code snippets for the current task
```

### 6. SkillHarvester â€” Reusable Functions
```python
# Extracts verified functions from successful code
# Saves as skills in skills/ directory
# Injects available skills into future prompts
```

### 7. ReflectionBuffer â€” Session Learning
```python
# Captures errors and lessons within a session
# Prevents repeating the same mistakes in refinement iterations
# Auto-generates lessons from common error types
```

---

## âš¡ Poetiq Pipeline Deep Dive

The Poetiq system implements **True Poetiq** â€” each worker:

### Phase 1: Parallel Generation
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                PoetiqRunner                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚Worker 0 â”‚  â”‚Worker 1 â”‚  â”‚Worker 2 â”‚         â”‚
â”‚  â”‚ t=0.3   â”‚  â”‚ t=0.5   â”‚  â”‚ t=0.7   â”‚ â† Variedâ”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   temps â”‚
â”‚       â”‚            â”‚            â”‚               â”‚
â”‚  [Generate Code]   [Generate]   [Generate]      â”‚
â”‚       â”‚            â”‚            â”‚               â”‚
â”‚  [Execute & Verify][Execute]   [Execute]        â”‚
â”‚       â”‚            â”‚            â”‚               â”‚
â”‚  [verified=True]  [verified=?] [verified=?]     â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                    â–¼                            â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚            â”‚  Aggregator  â”‚ â† Prioritizes       â”‚
â”‚            â”‚              â”‚   verified workers  â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 2: Self-Refine Loop
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SelfRefiner                         â”‚
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   score < 18?   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Evaluate â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Refine    â”‚  â”‚
â”‚  â”‚ (1 call) â”‚                 â”‚ (1 worker)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚       â–²                              â”‚          â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                  (max 3 iterations)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ˆ Adaptive Difficulty (Curriculum Learning)

The agent automatically adjusts task difficulty based on performance:

| Level | Name | Examples |
|-------|------|----------|
| 1 | Basic | reverse string, sum list, check even |
| 2 | Easy | count vowels, find max, remove duplicates |
| 3 | Medium | validate email, parse date, word frequency |
| 4 | Hard | merge intervals, balanced brackets, LRU cache |
| 5 | Expert | regex parser, expression evaluator, graph algorithms |

**Rules:**
- â‰¥75% success rate â†’ **Level Up** ğŸ“ˆ
- <40% success rate â†’ **Level Down** ğŸ“‰
- 30% chance to target weak categories

---

## ğŸ”§ Configuration

All settings are centralized in `config/settings.py`:

```python
# LLM Config
LLM_BASE_URL = "http://127.0.0.1:8080/v1"
LLM_MODEL = "local-model"
LLM_MAX_TOKENS = 2048
LLM_TEMPERATURE = 0.3

# Poetiq Config
POETIQ_NUM_WORKERS = 3
WORKER_TEMPS = [0.3, 0.5, 0.7]  # Diversity through temperature

# Self-Refine
REFINE_MAX_ITERATIONS = 3
REFINE_THRESHOLD = 18  # Score 0-25

# Memory
MEMORY_DECAY_FACTOR = 0.98
TOP_K_MEMORIES = 5

# And many more...
```

---

## ğŸ“Š LLM Call Efficiency

The system is optimized to minimize LLM calls:

| Phase | Calls | Notes |
|-------|-------|-------|
| Task Generation | 1 | Autonomous loop only |
| Workers (Ã—3) | 3-9 | 1 each, +retry if verify fails |
| Pre-Eval | 0-1 | **Skipped if all verified** âœ… |
| Refine (Ã—3 iter) | 3-6 | 1 eval + 1 refine per iter |
| Lesson Extract | 0-1 | **Skipped if high score** âœ… |
| **Total** | **~6-15** | (was ~30 before optimizations) |

---

## ğŸ§ª Running Tests

```bash
# Single task with Poetiq
python run_test.py "create a fibonacci function" --poetiq

# Stress test with multiple workers
python run_test.py --stress 6

# Run autonomous loop
python autonomous_loop.py
```

---

## ğŸ“œ Research & Inspiration

### Core Papers
| Paper | Contribution |
|-------|--------------|
| [Self-Refine](https://arxiv.org/abs/2303.17651) | Iterative refinement with self-feedback |
| [A-Mem](https://arxiv.org/abs/2502.12110) | Agentic memory with evolution & decay |
| [DreamCoder](https://arxiv.org/abs/2006.08381) | Program synthesis + skill library |

### Poetiq Architecture
- [Poetiq ARC-AGI Solver](https://github.com/poetiq-ai/poetiq-arc-agi-solver)
- [Traversing the Frontier](https://poetiq.ai/posts/arcagi_announcement/)
- [Shatters ARC-AGI-2](https://poetiq.ai/posts/arcagi_verified/)

### Tools
- [llama.cpp](https://github.com/ggerganov/llama.cpp) â€” Local inference server
- [ChromaDB](https://www.trychroma.com/) â€” Vector storage for memory

---

## ğŸ”’ Security

All file operations are sandboxed:
- Tools can only read/write within `sandbox/` directory
- Path traversal attacks are blocked
- Code execution is isolated

---

## ğŸ“‹ Requirements

**Python Packages** (see `requirements.txt`):
```
requests>=2.28.0
openai>=1.0.0
chromadb>=0.4.0
pandas>=2.0.0
numpy>=1.24.0
networkx>=3.0
flask>=2.3.0
```

**Hardware:**
- GPU with Vulkan/CUDA support (tested on AMD RX 6600)
- 16GB+ RAM recommended
- 50GB+ disk space for models

**LLM Server:**
- [llama.cpp](https://github.com/ggerganov/llama.cpp) compiled with Vulkan/CUDA
- Recommended model: Qwen2.5-Coder-7B-Instruct (Q4_K_M or Q5_K_M)

**Troubleshooting:**
- If ChromaDB fails: `pip install chromadb --upgrade`
- If GPU not detected: Check Vulkan/CUDA drivers
- If port 8080 busy: Change port in `config/settings.py`

---

## ğŸ¤ Contributing

Contributions welcome! Key areas:
- New tools for the agent
- Memory system improvements
- Performance optimizations
- Documentation

---

## ğŸ“„ License

MIT License â€” Use freely, learn boldly.

---

<div align="center">

**Built for self-improvement. By an agent, for agents (and curious humans).**

</div>
