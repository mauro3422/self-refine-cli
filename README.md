# Self-Refine CLI with Poetiq Parallel System

A self-refining AI agent powered by **llama.cpp** with true parallel inference on GPU.

## ğŸš€ Quick Start

```bash
# 1. Start the llama.cpp server (GPU)
start_server.bat

# 2. Run agent
python run_test.py "your task" --poetiq
```

## ğŸ—ï¸ Architecture

```
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   PoetiqRunner   â”‚  â† Orchestrator
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Worker 0 â”‚   â”‚Worker 1 â”‚   â”‚Worker 2 â”‚  â† 1 LLM call each (PARALLEL)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ VotingSystem â”‚  â† Pick best response
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ ToolExecutor â”‚  â† Execute winner's tool
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
self-refine-cli/
â”œâ”€â”€ core/                    # Core modules
â”‚   â”œâ”€â”€ llm_client.py       # llama.cpp client
â”‚   â”œâ”€â”€ poetiq.py           # Parallel workers system
â”‚   â”œâ”€â”€ agent.py            # Full self-refine agent
â”‚   â”œâ”€â”€ parsers.py          # Tool call extraction
â”‚   â”œâ”€â”€ prompts.py          # System prompts
â”‚   â”œâ”€â”€ evaluator.py        # Response evaluation
â”‚   â””â”€â”€ verification.py     # Code verification
â”œâ”€â”€ tools/                   # Agent tools
â”‚   â”œâ”€â”€ file_tools.py       # read_file, write_file, list_dir
â”‚   â””â”€â”€ command_tools.py    # python_exec, run_command
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py         # Configuration
â”œâ”€â”€ server/                  # llama.cpp binaries
â”œâ”€â”€ sandbox/                 # Agent workspace
â”œâ”€â”€ run_test.py             # Test runner
â”œâ”€â”€ start_server.bat        # Start GPU server
â””â”€â”€ stop_server.bat         # Stop server
```

## âš¡ Server Configuration

The llama.cpp server runs with:
- **6 parallel slots** for concurrent inference
- **Vulkan GPU** acceleration
- **16K context** window

## ğŸ§ª Usage

```bash
# Single agent
python run_test.py "list files in sandbox/"

# Parallel agents (3 workers, vote on best)
python run_test.py "create hello.py" --poetiq

# Parallel with 6 workers
python run_test.py "task" --poetiq -p 6

# Stress test
python run_test.py --stress 6
```

## ğŸ“Š Performance

| Mode | Time | 
|------|------|
| LM Studio (old) | ~5 min |
| **Poetiq + llama.cpp** | **~10s** |

## ğŸ› ï¸ Requirements

```
openai
requests
```

## ğŸ“œ Based on

- [Self-Refine Paper](https://arxiv.org/abs/2303.17651)
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
