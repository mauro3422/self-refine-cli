# üî¨ Evoluci√≥n Arquitect√≥nica: An√°lisis Cr√≠tico y Roadmap

> **Contexto**: An√°lisis de sugerencias externas (otra IA) vs estado real del sistema.
> **Fecha**: 9 Dic 2025

---

## ‚úÖ Lo que YA Tenemos (El reporte no lo sab√≠a)

| Sugerencia del Reporte | Estado Real | Implementaci√≥n |
|------------------------|-------------|----------------|
| "Slots para workers" | ‚úÖ **YA EXISTE** | `--parallel 3` en llama.cpp, 3 slots dedicados |
| "Workers paralelos" | ‚úÖ **YA EXISTE** | `LightWorker` con `ThreadPoolExecutor` |
| "Verificaci√≥n de c√≥digo" | ‚úÖ **YA EXISTE** | `generate_and_verify()` ejecuta y valida |
| "Memory Graph" | ‚úÖ **YA EXISTE** | `MemoryGraph` con NetworkX |
| "Skill Harvesting" | ‚úÖ **IMPLEMENTADO HOY** | `_learn_success_patterns()` en learner.py |
| "Skip cuando verificado" | ‚úÖ **IMPLEMENTADO HOY** | `SKIP SelfRefiner` cuando verified + score >= 15 |

---

## üî¥ Cr√≠tica: Problemas del An√°lisis Externo

### 1. "Context Thrashing" - **EXAGERADO**

El reporte menciona "1.954ms en reevaluar 1105 tokens". Pero:
- Nuestro modelo LFM2-1.2B es **recurrente** (no Transformer), no usa KV cache tradicional
- El contexto de 32K es suficiente para todo el pipeline
- Cada request usa ~2-4K tokens, no hay competencia real

**Veredicto**: No es un problema cr√≠tico con nuestro modelo espec√≠fico.

### 2. "LightWorker Monol√≠tico" - **PARCIALMENTE CORRECTO**

Tiene raz√≥n en que usamos el mismo prompt. PERO:
- Ya variamos temperaturas (0.3, 0.5, 0.7)
- El Aggregator es un rol diferente con prompt propio
- El SelfRefiner tiene su propio prompt cr√≠tico

**Mejora v√°lida**: S√≠ podr√≠amos agregar roles especializados.

### 3. "Memoria Reactiva" - **CORRECTO**

Es verdad que el grafo no hace inferencia proactiva. Solo recuperamos por similitud.

**Mejora v√°lida**: Implementar navegaci√≥n causal del grafo.

---

## üü¢ Ideas NUEVAS y √öTILES del Reporte

### 1. **Reflexion Buffer** (Prioridad: ALTA)
Persistir las reflexiones entre iteraciones para evitar repetir errores.

### 2. **Tree of Thoughts para ARC** (Prioridad: MEDIA)
En vez de 3 tareas distintas, 3 hip√≥tesis para la misma tarea.
Nuestro sistema YA tiene la infraestructura (`NUM_WORKERS=3`), solo falta cambiar el modo.

### 3. **Dynamic Tools Library** (Prioridad: MEDIA)
Cuando una funci√≥n verifica exitosamente, guardarla como herramienta reutilizable.

### 4. **Error Translation Layer** (Prioridad: ALTA)
Convertir tracebacks t√©cnicos en instrucciones sem√°nticas.

---

## üìã Roadmap Priorizado

### Fase 1: Quick Wins ‚úÖ
- [x] Skip SelfRefiner cuando verified 
- [x] Skip Execute cuando python_exec verific√≥
- [x] Patterns abstractos en learner.py
- [ ] **Error Translation Layer** ‚Üê PR√ìXIMO

### Fase 2: Reflexion
- [ ] Implementar `ReflectionBuffer` en el bucle de refine
- [ ] Persistir lecciones de sesi√≥n

### Fase 3: Especializaci√≥n
- [ ] Roles `ArchitectWorker`, `CoderWorker`, `ReviewerWorker`
- [ ] Pipeline: Plan ‚Üí Code ‚Üí Review ‚Üí Fix

### Fase 4: Memoria Proactiva
- [ ] Navegaci√≥n del Memory Graph antes de generar
- [ ] Inferencia abductiva

---

## üìä Resumen

~60% de las sugerencias del reporte ya las implementamos. Las ideas nuevas m√°s valiosas son:
1. **Error Translation** (f√°cil, alto impacto)
2. **Reflexion Buffer** (medio, alto impacto)
3. **Roles Especializados** (complejo, alto impacto)
