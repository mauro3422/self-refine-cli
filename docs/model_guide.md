# GuÃ­a del Modelo Local (Liquid LMF2)

Este documento explica cÃ³mo estÃ¡ configurado el modelo LLM local y las decisiones de diseÃ±o.

---

## ğŸ“‹ Por QuÃ© Usamos LLM Local (No APIs de Pago)

### Razones Principales:

| RazÃ³n | ExplicaciÃ³n |
|-------|-------------|
| **Costo** | APIs como OpenAI/Anthropic cobran por token. Para iteraciÃ³n continua (cientos de llamadas/dÃ­a), el costo es prohibitivo |
| **Privacidad** | Todo se ejecuta localmente. No hay datos enviados a servidores externos |
| **Velocidad de IteraciÃ³n** | Podemos hacer 10+ llamadas por minuto sin lÃ­mites de rate |
| **Hardware Disponible** | Este sistema estÃ¡ diseÃ±ado para GPUs de consumidor (AMD/NVIDIA) vÃ­a llama.cpp |
| **Aprendizaje Continuo** | El modelo puede fallar y aprender sin costo econÃ³mico |

### Trade-offs Aceptados:

- âŒ Modelo mÃ¡s pequeÃ±o = Menos "inteligente" que GPT-4/Claude
- âœ… PERO: IteraciÃ³n rÃ¡pida + Self-Refine compensa la diferencia
- âœ… PERO: Podemos hacer 100 intentos donde otros harÃ­an 1

---

## ğŸ”§ Modelo Actual: Liquid LMF2

### Especificaciones:
- **TamaÃ±o**: ~2.6B-3B parÃ¡metros (pequeÃ±o)
- **Ventaja**: Muy rÃ¡pido en hardware limitado
- **Debilidad**: No fue diseÃ±ado para tool-use nativo
- **Contexto**: Soporta hasta 32k tokens

### Limitaciones Conocidas:
1. **Alucina nombres de tools** - Inventa tools que no existen
2. **Razonamiento complejo limitado** - No maneja muchas instrucciones simultÃ¡neas
3. **Sensible a temperatura** - Temps altas (>0.5) causan mÃ¡s errores

---

## ğŸŒ¡ï¸ ConfiguraciÃ³n de Temperatura

### Â¿QuÃ© es la Temperatura?
La temperatura controla quÃ© tan "creativo" vs "determinÃ­stico" es el modelo:

| Temperatura | Efecto | Uso |
|-------------|--------|-----|
| 0.0-0.2 | Muy determinÃ­stico, repite patrones | EvaluaciÃ³n, verificaciÃ³n |
| 0.2-0.4 | Balanceado, menos errores | **Nuestro rango Ã³ptimo** |
| 0.5-0.7 | MÃ¡s creativo, mÃ¡s variaciÃ³n | GeneraciÃ³n de ideas |
| 0.8-1.0+ | Muy aleatorio, mÃ¡s alucinaciones | Evitar para tareas precisas |

### ConfiguraciÃ³n Actual (Optimizada para LMF2):
```python
# config/settings.py
TEMPERATURE = 0.3           # Default bajo para estabilidad
WORKER_TEMPS = [0.2, 0.3, 0.4]  # VariaciÃ³n mÃ­nima en paralelo
```

### Evidencia CientÃ­fica (2024):
- Research de ArXiv muestra que temperaturas altas aumentan probabilidad de alucinaciÃ³n
- Para modelos pequeÃ±os (7B o menos), se recomienda temp â‰¤ 0.3
- Temp=0 no es ideal porque puede atascarse en patrones repetitivos

---

## ğŸ’¬ CÃ³mo Hacer Prompts para LMF2

### Best Practices (De la documentaciÃ³n oficial de Liquid AI):

1. **Usar tokens `<think></think>` para Chain-of-Thought**:
   ```
   <think>
   - Â¿QuÃ© pide la tarea?
   - Â¿QuÃ© herramienta necesito?
   - Â¿QuÃ© parÃ¡metros?
   </think>
   
   {respuesta}
   ```

2. **Ser PRECISO y CONCISO**:
   - âŒ "PodrÃ­as tal vez considerar usar alguna herramienta para leer archivos..."
   - âœ… "Usa `read_file` con el parÃ¡metro `path`."

3. **Una instrucciÃ³n a la vez**:
   - âŒ "Lee el archivo, analÃ­zalo, extrae X, guÃ¡rdalo en Y, y haz un test"
   - âœ… "Lee el archivo X" â†’ (siguiente turno) â†’ "Analiza el contenido"

4. **Formato de salida explÃ­cito**:
   ```
   Responde en JSON:
   {"tool": "...", "params": {...}}
   ```

5. **Evitar instrucciones negativas complejas**:
   - âŒ "No uses herramientas que no estÃ©n en la lista ni inventes parÃ¡metros"
   - âœ… "Usa SOLO estas herramientas: read_file, write_file, list_dir"

---

## ğŸ”„ CÃ³mo Maneja el Sistema las Limitaciones

| LimitaciÃ³n | SoluciÃ³n Implementada |
|------------|----------------------|
| Alucina tools | `execute_tool()` sugiere tools existentes cuando falla |
| Pierde el mejor cÃ³digo | `SelfRefiner` guarda el mejor score, no el Ãºltimo |
| Connection errors | Retry con backoff exponencial (1s, 2s, 4s) |
| Servidor se cuelga | Health check + auto-restart |
| Olvida contexto | Memory Orchestrator inyecta memorias relevantes |

---

## ğŸ“Š ParÃ¡metros Recomendados por Liquid AI

```python
# RecomendaciÃ³n oficial de LMF2
temperature = 0.3
min_p = 0.15
repetition_penalty = 1.05
```

Nuestro sistema usa estos valores (excepto `min_p` que depende del servidor llama.cpp).

---

## ğŸ¯ Resumen

1. **Usamos LLM local** porque es gratis, privado, y permite iteraciÃ³n rÃ¡pida
2. **LMF2 es pequeÃ±o pero suficiente** con las tÃ©cnicas de Self-Refine
3. **Temperaturas bajas (0.2-0.4)** reducen alucinaciones
4. **Tokens `<think></think>`** mejoran el razonamiento del modelo
5. **El sistema compensa las limitaciones** con memoria, retry, y best-code tracking
